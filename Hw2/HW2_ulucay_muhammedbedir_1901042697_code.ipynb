{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1901042697 Muhammed Bedir ULUCAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from pprint import pprint\n",
    "from sklearn import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_risk = pd.read_csv(\"audit_data/audit_risk.csv\")\n",
    "trial = pd.read_csv(\"audit_data/trial.csv\")\n",
    "\n",
    "trial.columns = ['Sector_score','LOCATION_ID', 'PARA_A', 'Score_A', 'PARA_B', 'Score_B',  'TOTAL', 'numbers', 'Marks', 'Money_Value', 'MONEY_Marks', 'District', 'Loss', 'LOSS_SCORE', 'History', 'History_score', 'Score', 'Risk_trial' ]\n",
    "trial['Score_A'] = trial['Score_A']/10\n",
    "trial['Score_B'] = trial['Score_B']/10\n",
    "merged_df = pd.merge(audit_risk, trial, how='outer', on = ['History', 'LOCATION_ID', 'Money_Value', 'PARA_A', 'PARA_B', 'Score', 'Score_A', 'Score_B', 'Sector_score', 'TOTAL', 'numbers'])\n",
    "\n",
    "df = merged_df.drop(['Risk_trial', 'Detection_Risk', 'Risk_F'], axis = 1)\n",
    "df['Money_Value'] = df['Money_Value'].fillna(df['Money_Value'].median())\n",
    "df = df[(df.LOCATION_ID != 'LOHARU')]\n",
    "df = df[(df.LOCATION_ID != 'NUH')]\n",
    "df = df[(df.LOCATION_ID != 'SAFIDON')]\n",
    "df = df.astype(float)\n",
    "df = df.drop_duplicates(keep = 'first')\n",
    "df = df[['Risk_A', 'Risk_B', 'Risk_C', 'Risk_D','RiSk_E', 'Prob', 'Score', 'CONTROL_RISK', 'Audit_Risk', 'MONEY_Marks', 'Loss', 'Risk']]\n",
    "df = df.rename(columns={\"Risk\": \"label\"})\n",
    "audit_class_df = df.drop(\"Audit_Risk\", axis = 1)\n",
    "audit_class_df.info()\n",
    "audit_class_df.to_csv(\"audit_data/audit_clean_data.csv\", index=False)\n",
    "print(\"Updated number of rows in the dataset: \",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df = pd.read_csv('Bike-Sharing-Dataset/day.csv')\n",
    "hour_df = pd.read_csv('Bike-Sharing-Dataset/hour.csv')\n",
    "\n",
    "day_df.rename(columns={'instant':'id','dteday':'Date','yr':'Year','mnth':'Month',\n",
    "                       'weathersit':'WeatherCondition','atemp':'FeelinTemp',\n",
    "                       'hum':'Humidity','cnt':'label'},inplace=True)\n",
    "# hour_df.rename(columns={'instant':'id','dteday':'Date','yr':'Year','mnth':'Month','hr':'Hours', 'weathersit':'WeatherCondition','atemp':'FeelinTemp','hum':'Humidity','cnt':'TotalRentHourly'},inplace=True)\n",
    "hour_df.rename(columns={'instant':'rec_id','dteday':'datetime','holiday':'is_holiday',\n",
    "                     'workingday':'is_workingday', 'weathersit':'weather_condition',\n",
    "                     'hum':'humidity','mnth':'month', 'cnt':'label','hr':'hour',\n",
    "                     'yr':'year'},inplace=True)\n",
    "\n",
    "day_df['Date']=pd.to_datetime(day_df.Date)\n",
    "day_df['season']=day_df.season.astype('category')\n",
    "day_df['Year']=day_df.Year.astype('category')\n",
    "day_df['Month']=day_df.Month.astype('category')\n",
    "day_df['holiday']=day_df.holiday.astype('category')\n",
    "day_df['weekday']=day_df.weekday.astype('category')\n",
    "day_df['workingday']=day_df.workingday.astype('category')\n",
    "day_df['WeatherCondition']=day_df.WeatherCondition.astype('category')\n",
    "day_df = day_df.drop(['Date'], axis=1)\n",
    "\n",
    "hour_df['datetime']=pd.to_datetime(hour_df.datetime)\n",
    "hour_df['season']=hour_df.season.astype('category')\n",
    "hour_df['year']=hour_df.year.astype('category')\n",
    "hour_df['month']=hour_df.month.astype('category')\n",
    "hour_df['hour']=hour_df.hour.astype('category')\n",
    "hour_df['is_holiday']=hour_df.is_holiday.astype('category')\n",
    "hour_df['weekday']=hour_df.weekday.astype('category')\n",
    "hour_df['is_workingday']=hour_df.is_workingday.astype('category')\n",
    "hour_df['weather_condition']=hour_df.weather_condition.astype('category')\n",
    "hour_df = hour_df.drop(['datetime'], axis=1)\n",
    "\n",
    "print(\"DaysRents_df: {}\\nHourlyRents_df: {}\".format(day_df.shape, hour_df.shape))\n",
    "print(hour_df.columns)\n",
    "print(day_df.columns)\n",
    "\n",
    "hour_df.info()\n",
    "day_df.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size):\n",
    "    \n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(df))\n",
    "\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def classify_example(example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split()\n",
    "\n",
    "    # condition check\n",
    "    if example[feature_name] <= float(value):\n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    # iter branches\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return classify_example(example, residual_tree)\n",
    "    \n",
    "\n",
    "def calculate_accuracy(df, tree):\n",
    "\n",
    "    df[\"classification\"] = df.apply(classify_example, axis=1, args=(tree,))\n",
    "    df[\"classification_correct\"] = df[\"classification\"] == df[\"label\"]\n",
    "    \n",
    "    accuracy = df[\"classification_correct\"].mean()\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_purity(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def classify_data(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    return classification\n",
    "\n",
    "\n",
    "def get_potential_splits(data):\n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    for column_index in range(n_columns - 1):\n",
    "        potential_splits[column_index] = []\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "\n",
    "        for index in range(len(unique_values)):\n",
    "            if index != 0:\n",
    "                current_value = unique_values[index]\n",
    "                previous_value = unique_values[index - 1]\n",
    "                potential_split = (current_value + previous_value) / 2\n",
    "                \n",
    "                potential_splits[column_index].append(potential_split)\n",
    "    \n",
    "    return potential_splits\n",
    "\n",
    "\n",
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    data_below = data[split_column_values <= split_value]\n",
    "    data_above = data[split_column_values >  split_value]\n",
    "    \n",
    "    return data_below, data_above\n",
    "\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculate_overall_entropy(data_below, data_above):\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_entropy =  (p_data_below * calculate_entropy(data_below) \n",
    "                      + p_data_above * calculate_entropy(data_above))\n",
    "    \n",
    "    return overall_entropy\n",
    "\n",
    "\n",
    "def determine_best_split(data, potential_splits):\n",
    "    \n",
    "    overall_entropy = 9999\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n",
    "\n",
    "            if current_overall_entropy <= overall_entropy:\n",
    "                overall_entropy = current_overall_entropy\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "\n",
    "def decision_tree_algorithm(df, counter=0, min_samples=10, max_depth=15):\n",
    "    \n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df           \n",
    "    \n",
    "    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        classification = classify_data(data)\n",
    "        \n",
    "        return classification\n",
    "\n",
    "    else:    \n",
    "        counter += 1\n",
    "\n",
    "        potential_splits = get_potential_splits(data)\n",
    "        split_column, split_value = determine_best_split(data, potential_splits)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        \n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        question = \"{} <= {}\".format(feature_name, split_value)\n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        yes_answer = decision_tree_algorithm(data_below, counter, min_samples, max_depth)\n",
    "        no_answer = decision_tree_algorithm(data_above, counter, min_samples, max_depth)\n",
    "        \n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(X, y, attribute_types, options):\n",
    "    df = pd.DataFrame(X)\n",
    "    df['label'] = y\n",
    "    df = df.dropna()\n",
    "    tree = decision_tree_algorithm(df, max_depth=options['max_depth'], min_samples=options['min_samples'])\n",
    "    return tree\n",
    "\n",
    "\n",
    "def predict_df(dt, X, options):\n",
    "    accuracy = calculate_accuracy(X, dt)\n",
    "    return accuracy\n",
    "\n",
    "df = audit_class_df\n",
    "random.seed(0)\n",
    "train_df, test_df = train_test_split(df, test_size=50)\n",
    "\n",
    "\n",
    "options = {\n",
    "    'max_depth': 25,\n",
    "    'min_samples': 4,\n",
    "}\n",
    "attribute_types = {}\n",
    "tree = build_df(train_df.drop('label', axis=1), train_df['label'], None, options)\n",
    "accuracy = predict_df(tree, test_df, options)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "pprint(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_purity(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_leaf(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    leaf = np.mean(label_column)\n",
    "    \n",
    "    return leaf\n",
    "\n",
    "\n",
    "def get_potential_splits(data):\n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    for column_index in range(n_columns - 1):\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        \n",
    "        potential_splits[column_index] = unique_values\n",
    "    \n",
    "    return potential_splits\n",
    "\n",
    "\n",
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    type_of_feature = FEATURE_TYPES[split_column]\n",
    "    if type_of_feature == \"continuous\":\n",
    "        data_below = data[split_column_values <= split_value]\n",
    "        data_above = data[split_column_values >  split_value]\n",
    "    \n",
    "    else:\n",
    "        data_below = data[split_column_values == split_value]\n",
    "        data_above = data[split_column_values != split_value]\n",
    "    \n",
    "    return data_below, data_above\n",
    "\n",
    "\n",
    "def calculate_mse(data):\n",
    "    actual_values = data[:, -1]\n",
    "    if len(actual_values) == 0:   # empty data\n",
    "        mse = 0\n",
    "        \n",
    "    else:\n",
    "        prediction = np.mean(actual_values)\n",
    "        mse = np.mean((actual_values - prediction) **2)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculate_overall_metric(data_below, data_above, metric_function):\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_metric =  (p_data_below * metric_function(data_below) \n",
    "                     + p_data_above * metric_function(data_above))\n",
    "    \n",
    "    return overall_metric\n",
    "\n",
    "\n",
    "def determine_best_split(data, potential_splits):\n",
    "    \n",
    "    first_iteration = True\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            \n",
    "            current_overall_metric = calculate_overall_metric(data_below, data_above, metric_function=calculate_mse)\n",
    "\n",
    "            if first_iteration or current_overall_metric <= best_overall_metric:\n",
    "                first_iteration = False\n",
    "                \n",
    "                best_overall_metric = current_overall_metric\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "\n",
    "def determine_type_of_feature(df):\n",
    "    \n",
    "    feature_types = []\n",
    "    n_unique_values_treshold = 15\n",
    "    for feature in df.columns:\n",
    "        if feature != \"label\":\n",
    "            unique_values = df[feature].unique()\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "    \n",
    "    return feature_types\n",
    "\n",
    "\n",
    "def reg_decision_tree_algorithm(df, counter=0, min_samples=2, max_depth=5):\n",
    "    \n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS, FEATURE_TYPES\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        FEATURE_TYPES = determine_type_of_feature(df)\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df           \n",
    "    \n",
    "    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        leaf = create_leaf(data)\n",
    "        return leaf\n",
    "    \n",
    "    else:    \n",
    "        counter += 1\n",
    "\n",
    "        potential_splits = get_potential_splits(data)\n",
    "        split_column, split_value = determine_best_split(data, potential_splits)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        \n",
    "        if len(data_below) == 0 or len(data_above) == 0:\n",
    "            leaf = create_leaf(data)\n",
    "            return leaf\n",
    "        \n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        type_of_feature = FEATURE_TYPES[split_column]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            question = \"{} <= {}\".format(feature_name, split_value)\n",
    "            \n",
    "        # feature is categorical\n",
    "        else:\n",
    "            question = \"{} = {}\".format(feature_name, split_value)\n",
    "        \n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        yes_answer = reg_decision_tree_algorithm(data_below, counter, min_samples, max_depth)\n",
    "        no_answer = reg_decision_tree_algorithm(data_above, counter, min_samples, max_depth)\n",
    "        \n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree\n",
    "\n",
    "\n",
    "def random_forest_algorithm(train_df, n_trees, n_bootstrap, n_features, dt_max_depth):\n",
    "    forest = []\n",
    "    for i in range(n_trees):\n",
    "        df_bootstrapped = train_df.sample(n=n_bootstrap, replace=True)\n",
    "        tree = reg_decision_tree_algorithm(df_bootstrapped, max_depth=dt_max_depth)\n",
    "        forest.append(tree)\n",
    "    return forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rdf(X, y, attribute_types, N = 1, options = {\"max_depth\": 13, \"min_samples\": 2, \"max_features\": None}):\n",
    "    tree = reg_decision_tree_algorithm(X, max_depth=options[\"max_depth\"], min_samples=options[\"min_samples\"])\n",
    "    return tree\n",
    "\n",
    "\n",
    "def predict_df(dt, X, options):\n",
    "    accuracy = calculate_accuracy(X, dt)\n",
    "    return accuracy\n",
    "\n",
    "train_df, test_df = train_test_split(hour_df, test_size=15)\n",
    "\n",
    "options = {\n",
    "    \"max_depth\": 90,\n",
    "    \"min_samples\": 2,\n",
    "    \"max_features\": None,\n",
    "}\n",
    "\n",
    "tree = build_rdf(train_df, train_df['label'], None, 1, options=options)\n",
    "\n",
    "accuracy = predict_df(tree, test_df, options)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "pprint(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
